{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Necessary Libraries:\n-------","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2023-08-13T13:25:12.304376Z","iopub.execute_input":"2023-08-13T13:25:12.304782Z","iopub.status.idle":"2023-08-13T13:25:12.310710Z","shell.execute_reply.started":"2023-08-13T13:25:12.304753Z","shell.execute_reply":"2023-08-13T13:25:12.309276Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"## Load and Prepare Data:\n_______\nload the dataset using Pandas and prepare the features (X) and target variable (y).","metadata":{}},{"cell_type":"code","source":"def load_data(file_path):\n    data = pd.read_csv(file_path)\n    X = data[['age', 'avg_glucose_level']].values\n    y = data['stroke'].values\n    return X, y","metadata":{"execution":{"iopub.status.busy":"2023-08-13T13:00:57.712880Z","iopub.execute_input":"2023-08-13T13:00:57.714602Z","iopub.status.idle":"2023-08-13T13:00:57.721766Z","shell.execute_reply.started":"2023-08-13T13:00:57.714552Z","shell.execute_reply":"2023-08-13T13:00:57.720438Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Split Data into Training and Testing Sets:\n_____________\nUse the train_test_split function from Scikit-Learn to split the data into training and testing sets.","metadata":{}},{"cell_type":"code","source":"def split_data(X, y, test_size=0.2, random_state=42):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    return X_train, X_test, y_train, y_test","metadata":{"execution":{"iopub.status.busy":"2023-08-13T13:01:13.306791Z","iopub.execute_input":"2023-08-13T13:01:13.307230Z","iopub.status.idle":"2023-08-13T13:01:13.314237Z","shell.execute_reply.started":"2023-08-13T13:01:13.307197Z","shell.execute_reply":"2023-08-13T13:01:13.312759Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Implement Linear Regression:\n_____________\nDefine a function to implement linear regression using gradient descent.","metadata":{}},{"cell_type":"code","source":"def gradient_descent(X, y, learning_rate=0.01, num_iterations=1000):\n    m, n = X.shape\n    theta = np.zeros(n)\n    \n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        errors = y - predictions\n        gradient = -2/m * np.dot(X.T, errors)\n        \n        # Update theta with adaptive learning rate\n        learning_rate *= np.linalg.norm(gradient) / np.linalg.norm(X)  # Adaptive learning rate\n        theta -= learning_rate * gradient\n        \n    return theta","metadata":{"execution":{"iopub.status.busy":"2023-08-13T13:05:42.424666Z","iopub.execute_input":"2023-08-13T13:05:42.425124Z","iopub.status.idle":"2023-08-13T13:05:42.433456Z","shell.execute_reply.started":"2023-08-13T13:05:42.425091Z","shell.execute_reply":"2023-08-13T13:05:42.431904Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Train the Model:\n_____________\nCombine the data loading, splitting, and linear regression steps to train the model.","metadata":{}},{"cell_type":"code","source":"def train_model(X_train, y_train):\n    theta = gradient_descent(X_train, y_train)\n    return theta","metadata":{"execution":{"iopub.status.busy":"2023-08-13T13:05:43.872677Z","iopub.execute_input":"2023-08-13T13:05:43.873101Z","iopub.status.idle":"2023-08-13T13:05:43.878851Z","shell.execute_reply.started":"2023-08-13T13:05:43.873070Z","shell.execute_reply":"2023-08-13T13:05:43.877519Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate the Model:\n_____________\nDefine a function to evaluate the trained model on the testing set using Mean Squared Error.","metadata":{}},{"cell_type":"code","source":"def evaluate_model(theta, X_test, y_test):\n    predictions = np.dot(X_test, theta)\n    mse = mean_squared_error(y_test, predictions)\n    return mse","metadata":{"execution":{"iopub.status.busy":"2023-08-13T13:05:46.794267Z","iopub.execute_input":"2023-08-13T13:05:46.794712Z","iopub.status.idle":"2023-08-13T13:05:46.801515Z","shell.execute_reply.started":"2023-08-13T13:05:46.794665Z","shell.execute_reply":"2023-08-13T13:05:46.800042Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## Calculate accuracy\n_____________","metadata":{}},{"cell_type":"code","source":"# Additional function to calculate accuracy\ndef calculate_accuracy(predictions, threshold=0.5):\n    binary_predictions = np.where(predictions >= threshold, 1, 0)\n    return binary_predictions","metadata":{"execution":{"iopub.status.busy":"2023-08-13T13:07:03.425125Z","iopub.execute_input":"2023-08-13T13:07:03.425511Z","iopub.status.idle":"2023-08-13T13:07:03.431591Z","shell.execute_reply.started":"2023-08-13T13:07:03.425482Z","shell.execute_reply":"2023-08-13T13:07:03.430673Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## Putting it All Together:\n_____________","metadata":{}},{"cell_type":"code","source":"def main():\n    file_path = '/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv'\n    X, y = load_data(file_path)\n    X_train, X_test, y_train, y_test = split_data(X, y)\n    theta = train_model(X_train, y_train)\n    mse = evaluate_model(theta, X_test, y_test)\n    print(\"Mean Squared Error:\", mse)\n    binary_predictions = calculate_accuracy(predictions)\n    accuracy = accuracy_score(y_test, binary_predictions)\n    print(\"Accuracy:\", accuracy)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2023-08-13T13:25:14.409543Z","iopub.execute_input":"2023-08-13T13:25:14.409946Z","iopub.status.idle":"2023-08-13T13:25:14.488544Z","shell.execute_reply.started":"2023-08-13T13:25:14.409915Z","shell.execute_reply":"2023-08-13T13:25:14.487311Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Mean Squared Error: 0.05725120323739136\nAccuracy: 0.9393346379647749\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Inference\n______\n\nThe values indicate the performance of the linear regression model on the testing data. Let's break down the implications of the Mean Squared Error (MSE) and the Accuracy:\n\nMean Squared Error (MSE):\n\nThe MSE value shows 0.05725120323739136, which represents the average squared difference between the actual target values and the predicted values made by our linear regression model. A lower MSE indicates that the model's predictions are closer to the actual values, which is generally desirable. In this context, the MSE suggests that, on average, the squared difference between the predicted and actual stroke probabilities is relatively small.\n\nAccuracy:\n\nThe accuracy value shows 0.9393346379647749, which represents the proportion of correct predictions made by your model among all the predictions on the testing data. An accuracy of 0.9393 (or approximately 94%) indicates that our model correctly classified 94% of the instances in the testing set based on the predicted stroke probabilities. This is a measure of the model's overall correctness in terms of its predictions.\n\n**In summary:**\n\n**The low MSE value suggests that the model's predictions are reasonably close to the actual stroke probabilities.**\n\n**The high accuracy value indicates that the model's predictions are correct for around 94% of the instances in the testing set.**","metadata":{}}]}